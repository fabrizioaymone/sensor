{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMG Dataset Training and Inference with TCNN Network\n",
    "\n",
    "Made by STMicroelectronics\n",
    "\n",
    "\n",
    "Authors: Fabrizio Maria Aymone, Danilo Pau\n",
    "\n",
    "contact: danilo.pau@st.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download NinaPRO DB8 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --continue https://data.ncl.ac.uk/ndownloader/articles/9577598/versions/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip 1\n",
    "!mkdir ninaproDB8\n",
    "!unzip \\*.zip -d ninaproDB8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset and preprocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "linear_transformation_matrix = np.matrix([[0.639, 0.000, 0.000, 0.000, 0.000],\n",
    "                                                     [0.383, 0.000, 0.000, 0.000, 0.000],\n",
    "                                                     [0.000, 1.000, 0.000, 0.000, 0.000],\n",
    "                                                     [-0.639,0.000, 0.000, 0.000, 0.000],\n",
    "                                                     [0.000, 0.000, 0.400, 0.000, 0.000],\n",
    "                                                     [0.000, 0.000, 0.600, 0.000, 0.000],\n",
    "                                                     [0.000, 0.000, 0.000, 0.400, 0.000],\n",
    "                                                     [0.000, 0.000, 0.000, 0.600, 0.000],\n",
    "                                                     [0.000, 0.000, 0.000, 0.000, 0.000],\n",
    "                                                     [0.000, 0.000, 0.000, 0.000,0.1667],\n",
    "                                                     [0.000, 0.000, 0.000, 0.000,0.3333],\n",
    "                                                     [0.000, 0.000, 0.000, 0.000, 0.000],\n",
    "                                                     [0.000, 0.000, 0.000, 0.000,0.1667],\n",
    "                                                     [0.000, 0.000, 0.000, 0.000,0.3333],\n",
    "                                                     [0.000, 0.000, 0.000, 0.000, 0.000],\n",
    "                                                     [0.000, 0.000, 0.000, 0.000, 0.000],\n",
    "                                                     [-0.19, 0.000, 0.000, 0.000, 0.000],\n",
    "                                                     [0.000, 0.000, 0.000, 0.000, 0.000]\n",
    "                                                     ])\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def read_emg(emg_path):\n",
    "    # extract nth channel EMG, bandpass,  down-sampling, normalize\n",
    "    #b, a = signal.butter(4, [10,500], 'bp',fs=2000) #bandpass = signal.butter(4, [20,500], 'bandpass',output='sos',fs=2000)\n",
    "    emg_data = loadmat(emg_path)\n",
    "\n",
    "    X = emg_data.get('emg')\n",
    "    #X = signal.filtfilt(b,a,X,axis=0)\n",
    "    scale = 0.011*2/ (2**16-1) #the range of the Delsys Trigno is +-11 mV and the precision is int16\n",
    "    X = np.round(X/scale).astype(np.int16)\n",
    "    \n",
    "    y = emg_data.get('glove') @ linear_transformation_matrix\n",
    "   \n",
    "    y = y.astype('float32')\n",
    "\n",
    "    stimulus = emg_data.get('restimulus')\n",
    "    repetition = emg_data.get('rerepetition')\n",
    "\n",
    "    return X, y, stimulus, repetition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from itertools import chain\n",
    "import gc\n",
    "\n",
    "class CustomWindowGenerator(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, X, y, index_array=None, window_size=256, batch_size=64, slide=50, shuffle=True, pass_array=False):\n",
    "        \n",
    "        # X and y are lists of datasets e.g. if we have two datasets (X1, y1) and (X2, y2), X is [X1, X2] and y is [y1, y2]\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.w = window_size\n",
    "        self.batch_size = batch_size\n",
    "        self.slide = slide\n",
    "        self.shuffle = shuffle\n",
    "        self.index_array= list(chain(*[zip(np.repeat(idx, len(x)), np.arange(0, (len(x)-window_size+1), slide)) for idx, x in enumerate(X)])) if pass_array==False else index_array\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.index_array)\n",
    "        #self.epoch=1\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.index_array)\n",
    "        gc.collect()\n",
    "        keras.backend.clear_session()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        X_d = []\n",
    "        y_d = []\n",
    "        \n",
    "        for n, i in self.index_array[index*self.batch_size:index*self.batch_size + self.batch_size]:\n",
    "            \n",
    "            X_d.append([self.X[n][i:i+self.w]])\n",
    "            y_d.append(self.y[n][i+self.w-1])\n",
    "        \n",
    "        return np.vstack(X_d), np.vstack(y_d)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.index_array)//self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_for_user(s):\n",
    "    X_train_1, y_train_1, stimulus_1, repetition_1 = read_emg(f\"./ninaproDB8/S{s}_E1_A1.mat\")\n",
    "    X_train_2, y_train_2, stimulus_2, repetition_2 = read_emg(f\"./ninaproDB8/S{s}_E1_A2.mat\")\n",
    "    \n",
    "    \n",
    "    X_train, y_train, stimulus_train, repetition_train = [X_train_1, X_train_2], [y_train_1, y_train_2], [stimulus_1, stimulus_2], [repetition_1, repetition_2]\n",
    "    \n",
    "    X_test, y_test, _, _ = read_emg(f\"./ninaproDB8/S{s}_E1_A3.mat\")\n",
    "    X_test, y_test = [X_test], [y_test]\n",
    "\n",
    "    return X_train, y_train, stimulus_train, repetition_train, X_test, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "\n",
    "    model = keras.Sequential([\n",
    "\n",
    "                # BLOCK 1\n",
    "    \n",
    "                keras.layers.Input(shape=(256, 16)),\n",
    "                keras.layers.ZeroPadding1D(padding=((3-1)*2, 0)),\n",
    "                keras.layers.Conv1D(filters=16, kernel_size=3, padding='valid', strides=1, dilation_rate = 2, use_bias=False),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.ReLU(),\n",
    "                keras.layers.ZeroPadding1D(padding=((3-1)*2, 0)),\n",
    "                keras.layers.Conv1D(filters=16, kernel_size=3, padding='valid', strides=1, dilation_rate = 2, use_bias=False),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.ReLU(),\n",
    "                keras.layers.ZeroPadding1D(padding=((5-1)*1, 0)),\n",
    "                keras.layers.Conv1D(filters=16, kernel_size=5, padding='valid', strides=1, use_bias=False),\n",
    "                keras.layers.AveragePooling1D(pool_size=2, strides=2),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.ReLU(),\n",
    "                \n",
    "\n",
    "                # BLOCK 2\n",
    "\n",
    "                keras.layers.ZeroPadding1D(padding=((3-1)*4, 0)),\n",
    "                keras.layers.Conv1D(filters=32, kernel_size=3, padding='valid', strides=1, dilation_rate=4, use_bias=False),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.ReLU(),\n",
    "                keras.layers.ZeroPadding1D(padding=((3-1)*4, 0)),\n",
    "                keras.layers.Conv1D(filters=32, kernel_size=3, padding='valid', strides=1, dilation_rate=4, use_bias=False),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.ReLU(),\n",
    "                keras.layers.ZeroPadding1D(padding=((5-1)*1, 0)),\n",
    "                keras.layers.Conv1D(filters=32, kernel_size=5, padding='valid', strides=2, use_bias=False),\n",
    "                keras.layers.AveragePooling1D(pool_size=2, strides=2),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.ReLU(),\n",
    "\n",
    "                # BLOCK 3\n",
    "\n",
    "                keras.layers.ZeroPadding1D(padding=((3-1)*8, 0)),\n",
    "                keras.layers.Conv1D(filters=64, kernel_size=3, padding='valid', strides=1, dilation_rate=8, use_bias=False),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.ReLU(),\n",
    "                keras.layers.ZeroPadding1D(padding=((3-1)*8, 0)),\n",
    "                keras.layers.Conv1D(filters=64, kernel_size=3, padding='valid', strides=1, dilation_rate=8, use_bias=False),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.ReLU(),\n",
    "                keras.layers.ZeroPadding1D(padding=((5-1)*1, 0)),\n",
    "                keras.layers.Conv1D(filters=64, kernel_size=5, padding='valid', strides=4, use_bias=False),\n",
    "                keras.layers.AveragePooling1D(pool_size=2, strides=2),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.ReLU(),\n",
    "\n",
    "                # DENSE BLOCK\n",
    "                keras.layers.Flatten(),\n",
    "                keras.layers.Dense(4*64, use_bias=False),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.ReLU(),\n",
    "                keras.layers.Dropout(rate=0.5),\n",
    "                keras.layers.Dense(32, use_bias=False),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                keras.layers.ReLU(),\n",
    "                keras.layers.Dropout(rate=0.5),\n",
    "                keras.layers.Dense(5, use_bias=False)\n",
    "\n",
    "                ])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(train_gen, val_gen, checkpoint_filepath):\n",
    "\n",
    "    gc.collect()\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    \n",
    "    model = get_model()\n",
    "    \n",
    "    opt = keras.optimizers.Adam(learning_rate=10**(-4))\n",
    "\n",
    "    model.compile(loss=\"MeanAbsoluteError\", optimizer=opt)\n",
    "    \n",
    "    earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, verbose=0, mode='min')\n",
    "    reduce_lr_loss = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, min_delta=0.0001, mode='auto', cooldown=0, min_lr=0)\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_filepath, save_best_only=True, verbose=1, save_freq = \"epoch\", monitor='val_loss', mode='min')\n",
    "\n",
    "    model.fit(train_gen, validation_data = val_gen, epochs=50, callbacks=[reduce_lr_loss, checkpoint, earlyStopping])\n",
    "\n",
    "    return model\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras import *\n",
    "\n",
    "def get_qmodel():\n",
    "    \n",
    "    # check kernel constraint kernel_constraint=None\n",
    "\n",
    "    qmodel = keras.Sequential([\n",
    "\n",
    "        # BLOCK 1\n",
    "\n",
    "        keras.layers.Input(shape=(256, 16)),\n",
    "        keras.layers.ZeroPadding1D(padding=((3-1)*2, 0)),\n",
    "        QConv1D(filters=16, kernel_size=3, padding='valid', strides=1, dilation_rate = 2, kernel_quantizer=\"quantized_bits(8)\", use_bias=False),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        QActivation(\"quantized_relu(8)\"),\n",
    "        keras.layers.ZeroPadding1D(padding=((3-1)*2, 0)),\n",
    "        QConv1D(filters=16, kernel_size=3, padding='valid', strides=1, dilation_rate = 2, kernel_quantizer=\"quantized_bits(8)\", use_bias=False),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        QActivation(\"quantized_relu(8)\"),\n",
    "        keras.layers.ZeroPadding1D(padding=(5-1, 0)),\n",
    "        QConv1D(filters=16, kernel_size=5, padding='valid', strides=1, kernel_quantizer=\"quantized_bits(8)\", use_bias=False),\n",
    "        keras.layers.AveragePooling1D(pool_size=2, strides=2),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        QActivation(\"quantized_relu(8)\"),\n",
    "        \n",
    "        # BLOCK 2\n",
    "\n",
    "        keras.layers.ZeroPadding1D(padding=((3-1)*4, 0)),\n",
    "        QConv1D(filters=32, kernel_size=3, padding='valid', strides=1, dilation_rate=4, kernel_quantizer=\"quantized_bits(8)\", use_bias=False),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        QActivation(\"quantized_relu(8)\"),\n",
    "        keras.layers.ZeroPadding1D(padding=((3-1)*4, 0)),\n",
    "        QConv1D(filters=32, kernel_size=3, padding='valid', strides=1, dilation_rate=4, kernel_quantizer=\"quantized_bits(8)\", use_bias=False),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        QActivation(\"quantized_relu(8)\"),\n",
    "        keras.layers.ZeroPadding1D(padding=(5-1, 0)),\n",
    "        QConv1D(filters=32, kernel_size=5, padding='valid', strides=2, kernel_quantizer=\"quantized_bits(8)\", use_bias=False),\n",
    "        keras.layers.AveragePooling1D(pool_size=2, strides=2),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        QActivation(\"quantized_relu(8)\"),\n",
    "\n",
    "        # BLOCK 3\n",
    "\n",
    "        keras.layers.ZeroPadding1D(padding=((3-1)*8, 0)),\n",
    "        QConv1D(filters=64, kernel_size=3, padding='valid', strides=1, dilation_rate=8, kernel_quantizer=\"quantized_bits(8)\", use_bias=False),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        QActivation(\"quantized_relu(8)\"),\n",
    "        keras.layers.ZeroPadding1D(padding=((3-1)*8, 0)),\n",
    "        QConv1D(filters=64, kernel_size=3, padding='valid', strides=1, dilation_rate=8, kernel_quantizer=\"quantized_bits(8)\", use_bias=False),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        QActivation(\"quantized_relu(8)\"),\n",
    "        keras.layers.ZeroPadding1D(padding=(5-1, 0)),\n",
    "        QConv1D(filters=64, kernel_size=5, padding='valid', strides=4, kernel_quantizer=\"quantized_bits(8)\", use_bias=False),\n",
    "        keras.layers.AveragePooling1D(pool_size=2, strides=2),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        QActivation(\"quantized_relu(8)\"),\n",
    "\n",
    "        # DENSE BLOCK\n",
    "\n",
    "        keras.layers.Flatten(),\n",
    "        QDense(4*64, kernel_quantizer=\"quantized_bits(8)\", use_bias=False),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        QActivation(\"quantized_relu(8)\"),\n",
    "        keras.layers.Dropout(rate=0.5),\n",
    "        QDense(32, kernel_quantizer=\"quantized_bits(8)\", use_bias=False),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        QActivation(\"quantized_relu(8)\"),\n",
    "        keras.layers.Dropout(rate=0.5),\n",
    "        QDense(5, kernel_quantizer=\"quantized_bits(8)\", use_bias=False)\n",
    "        ])\n",
    "    \n",
    "    return qmodel\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras import *\n",
    "\n",
    "def fit_qmodel(train_gen, val_gen, checkpoint_filepath):\n",
    "\n",
    "    gc.collect()\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    \n",
    "    model = get_qmodel()\n",
    "    \n",
    "    opt = keras.optimizers.Adam(learning_rate=10**(-4))\n",
    "\n",
    "    model.compile(loss=\"MeanAbsoluteError\", optimizer=opt)\n",
    "    \n",
    "    earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, verbose=0, mode='min')\n",
    "    reduce_lr_loss = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, min_delta=0.0001, mode='auto', cooldown=0, min_lr=0)\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_filepath, save_best_only=True, verbose=1, save_freq = \"epoch\", monitor='val_loss', mode='min')\n",
    "\n",
    "    model.fit(train_gen, validation_data = val_gen, epochs=50, callbacks=[reduce_lr_loss, checkpoint, earlyStopping])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def get_metrics(predictions, y_test):\n",
    "    MAE = mean_absolute_error(predictions, np.array(y_test))\n",
    "\n",
    "    count_10 = 0\n",
    "    count_15 = 0\n",
    "\n",
    "    for i in range(0, len(predictions)):\n",
    "        for j in range(5):\n",
    "            if np.abs(predictions[i,j]-y_test[i,j]) <= 10:\n",
    "                count_10+=1\n",
    "            if np.abs(predictions[i,j]-y_test[i,j]) <= 15:\n",
    "                count_15+=1\n",
    "    \n",
    "    acc_10 = count_10/(len(predictions)*5)\n",
    "    acc_15 = count_15/(len(predictions)*5)\n",
    "\n",
    "    print(f\"MAE: {MAE}\")\n",
    "    print(f\"acc_10: {acc_10}\")\n",
    "    print(f\"acc_15: {acc_15}\")\n",
    "\n",
    "    return MAE, acc_10, acc_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_groups(stimulus):\n",
    "\n",
    "    a = 0\n",
    "    prev = 0\n",
    "\n",
    "    groups = np.zeros(len(stimulus))\n",
    "\n",
    "    for i in range(len(stimulus)):\n",
    "        groups[i] = a\n",
    "\n",
    "        if stimulus[i]==0:\n",
    "            a+=1\n",
    "\n",
    "    return groups      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "\n",
    "window_size = 256\n",
    "slide = 1\n",
    "batch_size = 256\n",
    "\n",
    "def train_user(s):\n",
    "    X_train, y_train, stimulus_train, repetition_train, X_test, y_test = prepare_dataset_for_user(s)\n",
    "    \n",
    "                                                                           \n",
    "    # scaler = StandardScaler(with_mean=True, with_std=True, copy=False).fit(np.vstack(X_train))\n",
    "    # X_train = [scaler.transform(x) for x in X_train]\n",
    "    # X_test = [scaler.transform(x) for x in X_test]\n",
    "    \n",
    "\n",
    "    test_gen = CustomWindowGenerator(X_test, y_test, window_size=256, batch_size=1363, slide=1, shuffle=False, pass_array=False)\n",
    "    \n",
    "\n",
    "    gkfold = sklearn.model_selection.StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "    index_array = np.array(list(chain(*[zip(np.repeat(idx, len(x)), np.arange(0, (len(x)-window_size+1), slide)) for idx, x in enumerate(X_train)])))\n",
    "\n",
    "    # stimulus for stratified k-folding\n",
    "    \n",
    "    stimulus_mock = []\n",
    "    for idx in range(len(index_array)):\n",
    "        stimulus_mock.append(stimulus_train[index_array[idx][0]][index_array[idx][1]])\n",
    "    stimulus_mock = np.vstack(stimulus_mock)\n",
    "\n",
    "    groups = get_groups(stimulus_mock)\n",
    "\n",
    "    # balance dataset according to stimulus (stimulus=0 is oversampled)\n",
    "    #index_array = index_array[np.where(stimulus_mock!=0)[0]]\n",
    "    #stimulus_mock = stimulus_mock[stimulus_mock!=0]\n",
    "    \n",
    "    #over_sampler = imblearn.over_sampling.RandomOverSampler(random_state=42)\n",
    "    #index_array, stimulus_mock = over_sampler.fit_resample(index_array, stimulus_mock)\n",
    "\n",
    "    \n",
    "    fold = 1\n",
    "    \n",
    "    for train_idx, val_idx in gkfold.split(X = np.zeros(len(index_array)), y = stimulus_mock, groups=groups):\n",
    "        \n",
    "        print(f\"\\n\\n*****************************************Fold: {fold}*****************************************\")\n",
    "        \n",
    "        train_gen, val_gen = CustomWindowGenerator(X_train, y_train, index_array = index_array[train_idx] ,window_size=window_size, batch_size=batch_size, slide=slide, shuffle=True, pass_array=True), \\\n",
    "                           CustomWindowGenerator(X_train, y_train, index_array = index_array[val_idx] ,window_size=window_size, batch_size=batch_size, slide=slide, shuffle=True, pass_array=True)\n",
    "\n",
    "        #----------------------------------------------- KERAS ----------------------------------------------------------------------------------------------\n",
    "\n",
    "        keras_models_dir = \"keras_models/\"\n",
    "        os.makedirs(keras_models_dir, exist_ok=True)\n",
    "        mcp_path = keras_models_dir+'keras_k_fold_'+str(fold)+'.h5'        \n",
    "        \n",
    "        model = fit_model(train_gen, val_gen, checkpoint_filepath=mcp_path)\n",
    "        model = keras.models.load_model(mcp_path)\n",
    "\n",
    "        predictions = model.predict(test_gen)\n",
    "        MAE, acc_10, acc_15 = get_metrics(predictions, y_test[0][window_size-1:])\n",
    "        print(f\"\\n\\n*****************************************Keras validation {fold} MAE: {MAE} acc_10: {acc_10}, acc_15: {acc_15} *****************************************\")\n",
    "\n",
    "\n",
    "        #----------------------------------------------- TFLITE ----------------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        def representative_data_gen():\n",
    "                repr_index_array = index_array\n",
    "                np.random.shuffle(repr_index_array)\n",
    "                for i in range(100):\n",
    "                        j = repr_index_array[i]\n",
    "                        yield [np.expand_dims(X_train[j[0]][j[1]:j[1]+window_size].astype(np.float32), axis=0)]\n",
    "        \n",
    "        \n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.representative_dataset = representative_data_gen\n",
    "        # Ensure that if any ops can't be quantized, the converter throws an error\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        # Set the input and output tensors to float32 and int8 (APIs added in r2.3)\n",
    "        converter.inference_input_type = tf.float32\n",
    "        converter.inference_output_type = tf.int8\n",
    "        tflite_model_quant = converter.convert()\n",
    "        \n",
    "        tflite_models_dir = \"tflite_models/\"\n",
    "        os.makedirs(tflite_models_dir, exist_ok=True)\n",
    "        tflite_model_path = tflite_models_dir+\"tflite_k_fold_\"+str(fold)+\".tflite\"\n",
    "        with open(tflite_model_path, 'wb') as f:\n",
    "                f.write(tflite_model_quant)\n",
    "\n",
    "        tflite_interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "        input_details = tflite_interpreter.get_input_details()\n",
    "        output_details = tflite_interpreter.get_output_details()\n",
    "        tflite_interpreter.resize_tensor_input(input_details[0]['index'], (1363, window_size, 16))\n",
    "        tflite_interpreter.allocate_tensors()\n",
    "        scale, zero_point = output_details[0][\"quantization\"]\n",
    "    \n",
    "        tflite_predictions = []\n",
    "\n",
    "        for X, _ in test_gen:\n",
    "            tflite_interpreter.set_tensor(input_details[0]['index'], X.astype(np.float32))\n",
    "            tflite_interpreter.invoke()\n",
    "            tflite_predictions.append(tflite_interpreter.get_tensor(output_details[0]['index']))\n",
    "        \n",
    "        tflite_predictions = np.vstack(tflite_predictions)\n",
    "        tflite_predictions = (tflite_predictions.astype(np.float32)-zero_point)*scale\n",
    "\n",
    "        MAE, acc_10, acc_15 = get_metrics(tflite_predictions, y_test[0][window_size-1:])\n",
    "        print(f\"\\n\\n*****************************************TFlite validation {fold} MAE: {MAE} acc_10: {acc_10}, acc_15: {acc_15} *****************************************\")\n",
    "\n",
    "\n",
    "        #----------------------------------------------- QKERAS ----------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "        qkeras_models_dir = \"qkeras_models/\"\n",
    "        os.makedirs(qkeras_models_dir, exist_ok=True)\n",
    "        mcp_path = qkeras_models_dir+'qkeras_k_fold_'+str(fold)+'.h5' \n",
    "        \n",
    "        qmodel = fit_qmodel(train_gen, val_gen, checkpoint_filepath=mcp_path)\n",
    "        \n",
    "        qmodel.load_weights(mcp_path)\n",
    "        \n",
    "        predictions = qmodel.predict(test_gen)\n",
    "        \n",
    "        MAE, acc_10, acc_15 = get_metrics(predictions, y_test[0][window_size-1:])\n",
    "        print(f\"\\n\\n*****************************************QKeras validation {fold} MAE: {MAE} acc_10: {acc_10}, acc_15: {acc_15} *****************************************\")\n",
    "        \n",
    "\n",
    "        gc.collect()\n",
    "        keras.backend.clear_session()\n",
    "        fold+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"./keras_models/keras_k_fold_1.h5\"\n",
    "model = keras.models.load_model(example)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"./qkeras_models/qkeras_k_fold_1.h5\"\n",
    "qmodel = get_qmodel()\n",
    "qmodel.load_weights(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_models_dir = \"keras_models/\"\n",
    "tflite_models_dir = \"tflite_models/\"\n",
    "qkeras_models_dir = \"qkeras_models/\"\n",
    "\n",
    "\n",
    "\n",
    "def test_models():\n",
    "\n",
    "    X_train, X_test, _, _, X_test, y_test = prepare_dataset_for_user(1)\n",
    "    \n",
    "    test_gen = CustomWindowGenerator(X_test, y_test, window_size=256, batch_size=1363, slide=1, shuffle=False, pass_array=False) #\n",
    "\n",
    "\n",
    "    results = {\"keras\":[], \"tflite\":[], \"qkeras\":[]}\n",
    "\n",
    "    #keras\n",
    "\n",
    "    model = get_model()\n",
    "    \n",
    "    for fold in range(1, 5+1):\n",
    "        path = keras_models_dir + \"keras_k_fold_\" + str(fold) + \".h5\"\n",
    "        model.load_weights(path)\n",
    "        predictions = model.predict(test_gen)\n",
    "        results[\"keras\"].append(list(get_metrics(predictions, y_test[0][window_size-1:])))\n",
    "    \n",
    "    #tflite\n",
    "    \n",
    "    for fold in range(1, 5+1):\n",
    "\n",
    "        path = tflite_models_dir + \"tflite_k_fold_\" + str(fold) + \".tflite\"\n",
    "        tflite_interpreter = tf.lite.Interpreter(model_path=path)\n",
    "        input_details = tflite_interpreter.get_input_details()\n",
    "        output_details = tflite_interpreter.get_output_details()\n",
    "        scale, zero_point = output_details[0]['quantization']\n",
    "\n",
    "        tflite_interpreter.resize_tensor_input(input_details[0]['index'], (1363, window_size, 16))\n",
    "        tflite_interpreter.allocate_tensors()\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        for X, _ in test_gen:\n",
    "            tflite_interpreter.set_tensor(input_details[0]['index'], X.astype(np.float32))\n",
    "            tflite_interpreter.invoke()\n",
    "            predictions.append(tflite_interpreter.get_tensor(output_details[0]['index']))\n",
    "        \n",
    "        predictions = np.vstack(predictions)\n",
    "        predictions = (predictions.astype(np.float32)-zero_point)*scale\n",
    "\n",
    "        results[\"tflite\"].append(list(get_metrics(predictions, y_test[0][window_size-1:])))\n",
    "    \n",
    "\n",
    "    #qkeras\n",
    "    \n",
    "    qmodel = get_qmodel()\n",
    "    \n",
    "    for fold in range(1, 5+1):\n",
    "        path = qkeras_models_dir + \"qkeras_k_fold_\" + str(fold) + \".h5\"\n",
    "        qmodel.load_weights(path)\n",
    "        predictions = qmodel.predict(test_gen)\n",
    "        results[\"qkeras\"].append(list(get_metrics(predictions, y_test[0][window_size-1:])))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "results = {\"keras\":[], \"tflite\":[], \"qkeras\":[]}\n",
    "\n",
    "results[\"keras\"] = np.load(\"./results/results_keras.npy\")\n",
    "results[\"tflite\"] = np.load(\"./results/results_tflite.npy\")\n",
    "results[\"qkeras\"] = np.load(\"./results/results_qkeras.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import scienceplots\n",
    "\n",
    "plt.style.use(['science','ieee', 'no-latex'])\n",
    "\n",
    "\n",
    "def plot_metrics(results):\n",
    "\n",
    "    keras_MAE = np.asarray(results[\"keras\"])[:,0]\n",
    "    tflite_MAE = np.asarray(results[\"tflite\"])[:,0]\n",
    "    qkeras_MAE = np.asarray(results[\"qkeras\"])[:,0]\n",
    "\n",
    "    keras_acc_10 = np.asarray(results[\"keras\"])[:,1]\n",
    "    tflite_acc_10 = np.asarray(results[\"tflite\"])[:,1]\n",
    "    qkeras_acc_10 = np.asarray(results[\"qkeras\"])[:,1]\n",
    "\n",
    "    keras_acc_15 = np.asarray(results[\"keras\"])[:,2]\n",
    "    tflite_acc_15 = np.asarray(results[\"tflite\"])[:,2]\n",
    "    qkeras_acc_15 = np.asarray(results[\"qkeras\"])[:,2]\n",
    "\n",
    "    print(qkeras_MAE)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12, 3))\n",
    "    fig.subplots_adjust(top=0.8)\n",
    "    x = [0, 1, 2]\n",
    "    labels = [\"Keras\", \"TFlite\", \"QKeras\"]\n",
    "\n",
    "    axs[0].errorbar(x, [np.average(keras_MAE), np.average(tflite_MAE), np.average(qkeras_MAE)], yerr=[np.std(keras_MAE), np.std(tflite_MAE), np.std(qkeras_MAE)], capsize=3, ms=3,fmt=\"r--o\", ecolor = \"black\")\n",
    "    axs[0].set_xticks(x, labels)\n",
    "    axs[0].set_title(\"MAE\")\n",
    "\n",
    "    print(np.average(keras_MAE), np.average(tflite_MAE), np.average(qkeras_MAE),np.std(keras_MAE), np.std(tflite_MAE), np.std(qkeras_MAE))\n",
    "\n",
    "    axs[1].errorbar(x, [np.average(keras_acc_10), np.average(tflite_acc_10), np.average(qkeras_acc_10)], yerr=[np.std(keras_acc_10), np.std(tflite_acc_10), np.std(qkeras_acc_10)], capsize=3, ms=3,fmt=\"r--o\", ecolor = \"black\")\n",
    "    axs[1].set_xticks(x, labels)\n",
    "    axs[1].set_title(\"Acc. 10$^\\circ$\")\n",
    "\n",
    "    print(np.average(keras_acc_10), np.average(tflite_acc_10), np.average(qkeras_acc_10), np.std(keras_acc_10), np.std(tflite_acc_10), np.std(qkeras_acc_10))\n",
    "\n",
    "    axs[2].errorbar(x, [np.average(keras_acc_15), np.average(tflite_acc_15), np.average(qkeras_acc_15)], yerr=[np.std(keras_acc_15), np.std(tflite_acc_15), np.std(qkeras_acc_15)], capsize=3, ms=3,fmt=\"r--o\", ecolor = \"black\")\n",
    "    axs[2].set_xticks(x, labels)\n",
    "    axs[2].set_title(\"Acc. 15$^\\circ$\")\n",
    "\n",
    "    print(np.average(keras_acc_15), np.average(tflite_acc_15), np.average(qkeras_acc_15), np.std(keras_acc_15), np.std(tflite_acc_15), np.std(qkeras_acc_15))\n",
    "\n",
    "    #fig.suptitle(\"TCN\", fontsize=16, y=0.95)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
